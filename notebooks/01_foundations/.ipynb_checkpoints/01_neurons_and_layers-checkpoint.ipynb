{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Neurons and Layers\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- What a neuron computes mathematically\n",
    "- How layers are built from neurons\n",
    "- The forward pass through a network\n",
    "- Implementing a neuron from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The Neuron: Basic Building Block\n",
    "\n",
    "A neuron computes:\n",
    "\n",
    "```\n",
    "output = activation(sum(inputs * weights) + bias)\n",
    "```\n",
    "\n",
    "Or mathematically:\n",
    "\n",
    "$$y = \\sigma\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x}$ = input vector\n",
    "- $\\mathbf{w}$ = weight vector\n",
    "- $b$ = bias term\n",
    "- $\\sigma$ = activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement a single neuron from scratch\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"A single artificial neuron.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, activation='relu'):\n",
    "        # Initialize weights randomly\n",
    "        self.weights = np.random.randn(n_inputs) * 0.1\n",
    "        self.bias = 0.0\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute neuron output.\"\"\"\n",
    "        # Weighted sum\n",
    "        z = np.dot(self.weights, x) + self.bias\n",
    "        \n",
    "        # Apply activation\n",
    "        if self.activation == 'relu':\n",
    "            return max(0, z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        else:\n",
    "            return z  # linear\n",
    "\n",
    "# Create a neuron with 3 inputs\n",
    "neuron = Neuron(n_inputs=3)\n",
    "print(f\"Weights: {neuron.weights}\")\n",
    "print(f\"Bias: {neuron.bias}\")\n",
    "\n",
    "# Test with some input\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "output = neuron.forward(x)\n",
    "print(f\"\\nInput: {x}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Exercise: Complete the Layer Implementation\n",
    "\n",
    "A layer is a collection of neurons. Complete the `forward` method below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"A layer of neurons.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs, activation='relu'):\n",
    "        # Weight matrix: (n_outputs, n_inputs)\n",
    "        self.weights = np.random.randn(n_outputs, n_inputs) * 0.1\n",
    "        self.biases = np.zeros(n_outputs)\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute layer output.\n",
    "        \n",
    "        TODO: Implement the forward pass\n",
    "        1. Compute z = Wx + b (matrix multiplication)\n",
    "        2. Apply activation function\n",
    "        3. Return the result\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # Hint: Use np.dot for matrix multiplication\n",
    "        z = np.dot(self.weights, x) + self.biases\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        else:\n",
    "            return z\n",
    "\n",
    "# Test your implementation\n",
    "layer = Layer(n_inputs=4, n_outputs=3)\n",
    "x = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "output = layer.forward(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity. Let's visualize the common ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Define activations\n",
    "relu = np.maximum(0, x)\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "tanh = np.tanh(x)\n",
    "leaky_relu = np.where(x > 0, x, 0.01 * x)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "axes[0,0].plot(x, relu)\n",
    "axes[0,0].set_title('ReLU: max(0, x)')\n",
    "axes[0,0].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[0,0].axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "\n",
    "axes[0,1].plot(x, sigmoid)\n",
    "axes[0,1].set_title('Sigmoid: 1/(1+e^-x)')\n",
    "axes[0,1].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "axes[1,0].plot(x, tanh)\n",
    "axes[1,0].set_title('Tanh')\n",
    "axes[1,0].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "\n",
    "axes[1,1].plot(x, leaky_relu)\n",
    "axes[1,1].set_title('Leaky ReLU')\n",
    "axes[1,1].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Using PyTorch\n",
    "\n",
    "Now let's see how PyTorch makes this easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch layer\n",
    "torch_layer = nn.Linear(in_features=4, out_features=3)\n",
    "\n",
    "# Input (needs to be a tensor)\n",
    "x_torch = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "# Forward pass\n",
    "output_torch = torch_layer(x_torch)\n",
    "\n",
    "print(f\"PyTorch layer weights shape: {torch_layer.weight.shape}\")\n",
    "print(f\"Output: {output_torch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. A **neuron** computes a weighted sum plus bias, then applies an activation\n",
    "2. A **layer** is multiple neurons processing the same input in parallel\n",
    "3. **Activation functions** introduce non-linearity (essential for learning complex patterns)\n",
    "4. **PyTorch** provides optimized implementations of these building blocks\n",
    "\n",
    "**Next:** Continue to `02_forward_pass.ipynb` to learn how data flows through networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
