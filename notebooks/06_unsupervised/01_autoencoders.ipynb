{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: Unsupervised Learning - Autoencoders\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand representation learning without labels\n",
    "- Implement basic and variational autoencoders\n",
    "- Visualize learned latent spaces\n",
    "- Connect autoencoders to sparse representations\n",
    "\n",
    "**Prerequisites:** Modules 01-05 (Neural networks, supervised learning, graph basics, topology, sparse networks)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Unsupervised Learning\n",
    "\n",
    "So far, we've worked with **supervised learning** where we have input-output pairs (X, y).\n",
    "\n",
    "In **unsupervised learning**, we only have inputs X and want to discover:\n",
    "- Hidden structure in the data\n",
    "- Compressed representations\n",
    "- Generative models\n",
    "\n",
    "**Why does this matter for our architecture?**\n",
    "- Multi-modal learning often requires learning shared representations\n",
    "- Sparse autoencoders connect to our topology work\n",
    "- Representation learning is key to cross-modal binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"[OK] Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Autoencoder Concept\n",
    "\n",
    "An autoencoder learns to:\n",
    "1. **Encode**: Compress input X into a smaller latent representation Z\n",
    "2. **Decode**: Reconstruct input X' from Z\n",
    "\n",
    "```\n",
    "Input X  -->  [Encoder]  -->  Latent Z  -->  [Decoder]  -->  Output X'\n",
    "(784)           |             (32)              |            (784)\n",
    "                |                               |\n",
    "           Compression                    Reconstruction\n",
    "```\n",
    "\n",
    "The key insight: if X' is close to X, then Z must capture the essential information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST for experiments\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='../../data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"[OK] Loaded {len(train_dataset)} training images\")\n",
    "print(f\"[OK] Loaded {len(test_dataset)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing a Basic Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicAutoencoder(nn.Module):\n",
    "    \"\"\"Simple fully-connected autoencoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=784, hidden_dim=256, latent_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: compress input to latent space\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, latent_dim),\n",
    "        )\n",
    "        \n",
    "        # Decoder: reconstruct from latent space\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid(),  # Output in [0, 1] range\n",
    "        )\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent representation.\"\"\"\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent representation to reconstruction.\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Full forward pass: encode then decode.\"\"\"\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, z\n",
    "\n",
    "# Create model\n",
    "autoencoder = BasicAutoencoder(latent_dim=32).to(device)\n",
    "print(f\"[OK] Created autoencoder with {sum(p.numel() for p in autoencoder.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Autoencoder\n",
    "\n",
    "The loss function is **reconstruction error**: how different is X' from X?\n",
    "\n",
    "Common choices:\n",
    "- **MSE Loss**: Mean squared error (good for continuous data)\n",
    "- **BCE Loss**: Binary cross-entropy (good for binary/image data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, train_loader, epochs=10, lr=0.001):\n",
    "    \"\"\"Train autoencoder with reconstruction loss.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = {'loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            # Flatten images\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            recon, z = model(data)\n",
    "            loss = criterion(recon, data)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        history['loss'].append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train the model\n",
    "print(\"Training autoencoder...\")\n",
    "history = train_autoencoder(autoencoder, train_loader, epochs=10)\n",
    "print(\"[OK] Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history['loss'], 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Reconstruction Loss')\n",
    "plt.title('Autoencoder Training')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions(model, test_loader, n_samples=10):\n",
    "    \"\"\"Show original images and their reconstructions.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch\n",
    "    data, labels = next(iter(test_loader))\n",
    "    data = data[:n_samples]\n",
    "    data_flat = data.view(data.size(0), -1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        recon, z = model(data_flat)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, n_samples, figsize=(15, 3))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Original\n",
    "        axes[0, i].imshow(data[i].squeeze().numpy(), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title('Original', fontsize=10)\n",
    "        \n",
    "        # Reconstruction\n",
    "        axes[1, i].imshow(recon[i].view(28, 28).cpu().numpy(), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title('Reconstructed', fontsize=10)\n",
    "    \n",
    "    plt.suptitle('Autoencoder Reconstructions', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_reconstructions(autoencoder, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploring the Latent Space\n",
    "\n",
    "The latent space Z is where the magic happens. Let's visualize it using t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_latent_space(model, test_loader, n_samples=2000):\n",
    "    \"\"\"Visualize latent space using t-SNE.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    latents = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            z = model.encode(data)\n",
    "            latents.append(z.cpu().numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "            \n",
    "            if sum(len(l) for l in latents) >= n_samples:\n",
    "                break\n",
    "    \n",
    "    latents = np.concatenate(latents)[:n_samples]\n",
    "    labels_arr = np.concatenate(labels_list)[:n_samples]\n",
    "    \n",
    "    # t-SNE projection\n",
    "    print(\"Computing t-SNE projection...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    latents_2d = tsne.fit_transform(latents)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(latents_2d[:, 0], latents_2d[:, 1], \n",
    "                          c=labels_arr, cmap='tab10', alpha=0.6, s=10)\n",
    "    plt.colorbar(scatter, label='Digit')\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "    plt.title('Latent Space Visualization (t-SNE)')\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space(autoencoder, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sparse Autoencoder\n",
    "\n",
    "Now let's connect this to our sparse network work! A **sparse autoencoder** encourages the latent representation to be sparse (mostly zeros).\n",
    "\n",
    "This is done by adding a **sparsity penalty** to the loss:\n",
    "\n",
    "```\n",
    "Loss = Reconstruction_Loss + lambda * Sparsity_Penalty\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(nn.Module):\n",
    "    \"\"\"Autoencoder with sparse latent representation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=784, hidden_dim=256, latent_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            # No activation - we'll apply sparsity constraint\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, z\n",
    "\n",
    "\n",
    "def sparsity_penalty(z, target_sparsity=0.05):\n",
    "    \"\"\"KL divergence sparsity penalty.\n",
    "    \n",
    "    Encourages average activation to match target_sparsity.\n",
    "    \"\"\"\n",
    "    # Average activation per neuron across batch\n",
    "    rho_hat = torch.sigmoid(z).mean(dim=0)\n",
    "    rho = torch.tensor(target_sparsity).to(z.device)\n",
    "    \n",
    "    # KL divergence\n",
    "    kl = rho * torch.log(rho / (rho_hat + 1e-8)) + \\\n",
    "         (1 - rho) * torch.log((1 - rho) / (1 - rho_hat + 1e-8))\n",
    "    \n",
    "    return kl.sum()\n",
    "\n",
    "\n",
    "def train_sparse_autoencoder(model, train_loader, epochs=10, lr=0.001, \n",
    "                              sparsity_weight=0.1, target_sparsity=0.05):\n",
    "    \"\"\"Train with sparsity constraint.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = {'loss': [], 'recon_loss': [], 'sparsity_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_recon = 0\n",
    "        total_sparse = 0\n",
    "        \n",
    "        for data, _ in train_loader:\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            \n",
    "            recon, z = model(data)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            recon_loss = criterion(recon, data)\n",
    "            \n",
    "            # Sparsity penalty\n",
    "            sparse_loss = sparsity_penalty(z, target_sparsity)\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = recon_loss + sparsity_weight * sparse_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_recon += recon_loss.item()\n",
    "            total_sparse += sparse_loss.item()\n",
    "        \n",
    "        n_batches = len(train_loader)\n",
    "        history['loss'].append(total_loss / n_batches)\n",
    "        history['recon_loss'].append(total_recon / n_batches)\n",
    "        history['sparsity_loss'].append(total_sparse / n_batches)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/n_batches:.4f} \"\n",
    "              f\"(Recon: {total_recon/n_batches:.4f}, Sparse: {total_sparse/n_batches:.4f})\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train sparse autoencoder\n",
    "sparse_ae = SparseAutoencoder(latent_dim=64).to(device)\n",
    "print(\"\\nTraining sparse autoencoder...\")\n",
    "sparse_history = train_sparse_autoencoder(sparse_ae, train_loader, epochs=10)\n",
    "print(\"[OK] Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sparsity(model1, model2, test_loader, name1='Basic', name2='Sparse'):\n",
    "    \"\"\"Compare activation sparsity of two models.\"\"\"\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    \n",
    "    data, _ = next(iter(test_loader))\n",
    "    data = data.view(data.size(0), -1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z1 = model1.encode(data)\n",
    "        z2 = model2.encode(data)\n",
    "    \n",
    "    # Compute sparsity (fraction of near-zero activations)\n",
    "    threshold = 0.1\n",
    "    sparsity1 = (torch.abs(z1) < threshold).float().mean().item()\n",
    "    sparsity2 = (torch.abs(torch.sigmoid(z2) - 0.5) > 0.4).float().mean().item()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Histogram of activations\n",
    "    axes[0].hist(z1.cpu().numpy().flatten(), bins=50, alpha=0.7, label=name1)\n",
    "    axes[0].hist(z2.cpu().numpy().flatten(), bins=50, alpha=0.7, label=name2)\n",
    "    axes[0].set_xlabel('Activation Value')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Latent Activation Distribution')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Sample latent vectors as heatmap\n",
    "    combined = torch.cat([torch.sigmoid(z1[:8]), torch.sigmoid(z2[:8])], dim=0)\n",
    "    im = axes[1].imshow(combined.cpu().numpy(), aspect='auto', cmap='viridis')\n",
    "    axes[1].set_xlabel('Latent Dimension')\n",
    "    axes[1].set_ylabel('Sample')\n",
    "    axes[1].set_title('Latent Activations (top: Basic, bottom: Sparse)')\n",
    "    axes[1].axhline(y=7.5, color='red', linestyle='--', linewidth=2)\n",
    "    plt.colorbar(im, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"{name1} model - Near-zero activations: {sparsity1*100:.1f}%\")\n",
    "    print(f\"{name2} model - Sparse activations: {sparsity2*100:.1f}%\")\n",
    "\n",
    "compare_sparsity(autoencoder, sparse_ae, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercise: Implement a Convolutional Autoencoder\n",
    "\n",
    "**TODO:** Complete the convolutional autoencoder below. This architecture preserves spatial structure better than fully-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    \"\"\"Convolutional autoencoder for image data.\n",
    "    \n",
    "    TODO: Complete the encoder and decoder architectures.\n",
    "    \n",
    "    Encoder should:\n",
    "    - Use Conv2d layers to downsample\n",
    "    - End with a flatten and linear to latent_dim\n",
    "    \n",
    "    Decoder should:\n",
    "    - Start with linear from latent_dim\n",
    "    - Use ConvTranspose2d to upsample\n",
    "    - Output same size as input (1, 28, 28)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Implement encoder\n",
    "        # Hint: Conv2d(1, 32, 3, stride=2, padding=1) -> (32, 14, 14)\n",
    "        #       Conv2d(32, 64, 3, stride=2, padding=1) -> (64, 7, 7)\n",
    "        #       Flatten -> Linear(64*7*7, latent_dim)\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Your code here\n",
    "            nn.Identity()  # Placeholder\n",
    "        )\n",
    "        \n",
    "        # TODO: Implement decoder\n",
    "        # Hint: Linear(latent_dim, 64*7*7) -> Reshape(64, 7, 7)\n",
    "        #       ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1)\n",
    "        #       ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Your code here\n",
    "            nn.Identity()  # Placeholder\n",
    "        )\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "# Test your implementation\n",
    "# conv_ae = ConvAutoencoder(latent_dim=32).to(device)\n",
    "# test_input = torch.randn(4, 1, 28, 28).to(device)\n",
    "# output, latent = conv_ae(test_input)\n",
    "# print(f\"Input shape: {test_input.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")\n",
    "# print(f\"Latent shape: {latent.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Connection to Multi-Modal Learning\n",
    "\n",
    "Autoencoders are foundational for multi-modal learning because:\n",
    "\n",
    "1. **Shared Latent Space**: Different modalities can be encoded to the same latent space\n",
    "2. **Cross-Modal Translation**: Decode from one modality's latent to another's output\n",
    "3. **Representation Alignment**: Learn representations that align across modalities\n",
    "\n",
    "In our capstone architecture, each modality encoder can be thought of as the encoder half of an autoencoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview: Multi-modal autoencoder concept\n",
    "print(\"\"\"\n",
    "Multi-Modal Autoencoder Architecture:\n",
    "\n",
    "    Image  -->  [Visual Encoder]  --+\n",
    "                                    |                 +--> [Visual Decoder] --> Image'\n",
    "    Audio  -->  [Audio Encoder]   --+--> [Shared Z] --+\n",
    "                                    |                 +--> [Audio Decoder] --> Audio'\n",
    "    Text   -->  [Text Encoder]    --+                 +--> [Text Decoder]  --> Text'\n",
    "\n",
    "The shared latent space Z enables:\n",
    "- Cross-modal retrieval (find audio matching an image)\n",
    "- Modal translation (generate image from text)\n",
    "- Joint representation learning\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Autoencoders** learn compressed representations without labels\n",
    "2. **Latent space** captures essential data structure\n",
    "3. **Sparse autoencoders** encourage interpretable, sparse representations\n",
    "4. **Connection to multi-modal**: Each modality encoder learns a representation\n",
    "\n",
    "**Next Module:** We'll dive into dynamic sparse training with SET and DEEP R algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "**[->] Continue to Module 07: Dynamic Sparse Training**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
