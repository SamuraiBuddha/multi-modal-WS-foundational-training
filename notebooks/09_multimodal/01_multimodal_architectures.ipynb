{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 09: Multi-Modal Architectures\n",
    "\n",
    "Multi-modal learning combines information from different data modalities (vision, text, audio).\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand modality-specific encoders\n",
    "- Learn fusion strategies (early, late, hybrid)\n",
    "- Implement cross-modal attention\n",
    "- Connect multi-modal fusion with Watts-Strogatz topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"[OK] Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-Modal Data\n",
    "\n",
    "Different modalities capture different aspects of the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multi-modal data\n",
    "batch_size = 8\n",
    "\n",
    "# Visual: e.g., image features (could be from CNN)\n",
    "visual = torch.randn(batch_size, 3, 32, 32)  # 3-channel 32x32 images\n",
    "\n",
    "# Text: e.g., word embeddings (sequence of tokens)\n",
    "text = torch.randn(batch_size, 20, 128)  # 20 tokens, 128-dim embeddings\n",
    "\n",
    "# Audio: e.g., mel spectrogram features\n",
    "audio = torch.randn(batch_size, 1, 64, 100)  # 64 mel bands, 100 time frames\n",
    "\n",
    "print(\"Multi-Modal Input Shapes:\")\n",
    "print(f\"  Visual: {visual.shape} (batch, channels, height, width)\")\n",
    "print(f\"  Text: {text.shape} (batch, sequence, embedding)\")\n",
    "print(f\"  Audio: {audio.shape} (batch, channels, freq, time)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modality-Specific Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    \"\"\"Encode visual features using CNN.\"\"\"\n",
    "    def __init__(self, output_dim=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(64, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Encode text features using Transformer-style attention.\"\"\"\n",
    "    def __init__(self, input_dim=128, output_dim=256):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(input_dim, num_heads=4, batch_first=True)\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Self-attention over sequence\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        # Pool over sequence\n",
    "        pooled = attn_out.mean(dim=1)\n",
    "        return self.fc(pooled)\n",
    "\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    \"\"\"Encode audio features using CNN.\"\"\"\n",
    "    def __init__(self, output_dim=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(64, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Test encoders\n",
    "visual_enc = VisualEncoder(256)\n",
    "text_enc = TextEncoder(128, 256)\n",
    "audio_enc = AudioEncoder(256)\n",
    "\n",
    "v_feat = visual_enc(visual)\n",
    "t_feat = text_enc(text)\n",
    "a_feat = audio_enc(audio)\n",
    "\n",
    "print(\"Encoded Feature Shapes:\")\n",
    "print(f\"  Visual: {v_feat.shape}\")\n",
    "print(f\"  Text: {t_feat.shape}\")\n",
    "print(f\"  Audio: {a_feat.shape}\")\n",
    "print(\"\\n[OK] All modalities encoded to same dimension (256)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fusion Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyFusion(nn.Module):\n",
    "    \"\"\"Concatenate features early and process together.\"\"\"\n",
    "    def __init__(self, modality_dims, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        total_dim = sum(modality_dims)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(total_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # features: list of tensors [visual, text, audio]\n",
    "        concat = torch.cat(features, dim=-1)\n",
    "        return self.fusion(concat)\n",
    "\n",
    "\n",
    "class LateFusion(nn.Module):\n",
    "    \"\"\"Process each modality separately, combine predictions.\"\"\"\n",
    "    def __init__(self, modality_dim, num_modalities, output_dim):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(modality_dim, output_dim)\n",
    "            for _ in range(num_modalities)\n",
    "        ])\n",
    "        self.weights = nn.Parameter(torch.ones(num_modalities) / num_modalities)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # Each modality makes its own prediction\n",
    "        predictions = [head(feat) for head, feat in zip(self.heads, features)]\n",
    "        # Weighted average\n",
    "        weights = F.softmax(self.weights, dim=0)\n",
    "        combined = sum(w * p for w, p in zip(weights, predictions))\n",
    "        return combined\n",
    "\n",
    "\n",
    "class HybridFusion(nn.Module):\n",
    "    \"\"\"Combine early fusion with modality-specific processing.\"\"\"\n",
    "    def __init__(self, modality_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Per-modality refinement\n",
    "        self.refine = nn.ModuleList([\n",
    "            nn.Linear(modality_dim, modality_dim) for _ in range(3)\n",
    "        ])\n",
    "        # Joint processing\n",
    "        self.joint = nn.Sequential(\n",
    "            nn.Linear(modality_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features):\n",
    "        refined = [F.relu(r(f)) for r, f in zip(self.refine, features)]\n",
    "        concat = torch.cat(refined, dim=-1)\n",
    "        return self.joint(concat)\n",
    "\n",
    "\n",
    "# Test fusion strategies\n",
    "features = [v_feat, t_feat, a_feat]\n",
    "\n",
    "early = EarlyFusion([256, 256, 256], 256, 10)\n",
    "late = LateFusion(256, 3, 10)\n",
    "hybrid = HybridFusion(256, 256, 10)\n",
    "\n",
    "print(\"Fusion Output Shapes:\")\n",
    "print(f\"  Early Fusion: {early(features).shape}\")\n",
    "print(f\"  Late Fusion: {late(features).shape}\")\n",
    "print(f\"  Hybrid Fusion: {hybrid(features).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Modal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention from one modality to another.\n",
    "    Query from modality A, Key/Value from modality B.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "    \n",
    "    def forward(self, query_modality, key_value_modality):\n",
    "        # Add sequence dimension if needed\n",
    "        if query_modality.dim() == 2:\n",
    "            query_modality = query_modality.unsqueeze(1)\n",
    "        if key_value_modality.dim() == 2:\n",
    "            key_value_modality = key_value_modality.unsqueeze(1)\n",
    "        \n",
    "        attn_out, attn_weights = self.attention(\n",
    "            query_modality, \n",
    "            key_value_modality, \n",
    "            key_value_modality\n",
    "        )\n",
    "        return attn_out.squeeze(1), attn_weights\n",
    "\n",
    "\n",
    "class MultiModalTransformer(nn.Module):\n",
    "    \"\"\"Full multi-modal fusion with cross-attention.\"\"\"\n",
    "    def __init__(self, dim=256, num_heads=4, output_dim=10):\n",
    "        super().__init__()\n",
    "        # Cross-modal attention: each modality attends to others\n",
    "        self.v_to_t = CrossModalAttention(dim, num_heads)\n",
    "        self.v_to_a = CrossModalAttention(dim, num_heads)\n",
    "        self.t_to_v = CrossModalAttention(dim, num_heads)\n",
    "        self.t_to_a = CrossModalAttention(dim, num_heads)\n",
    "        self.a_to_v = CrossModalAttention(dim, num_heads)\n",
    "        self.a_to_t = CrossModalAttention(dim, num_heads)\n",
    "        \n",
    "        # Fusion\n",
    "        self.fusion = nn.Linear(dim * 3, output_dim)\n",
    "    \n",
    "    def forward(self, visual, text, audio):\n",
    "        # Cross-modal attention\n",
    "        v_from_t, _ = self.v_to_t(visual, text)\n",
    "        v_from_a, _ = self.v_to_a(visual, audio)\n",
    "        v_enhanced = visual + v_from_t + v_from_a\n",
    "        \n",
    "        t_from_v, _ = self.t_to_v(text, visual)\n",
    "        t_from_a, _ = self.t_to_a(text, audio)\n",
    "        t_enhanced = text + t_from_v + t_from_a\n",
    "        \n",
    "        a_from_v, _ = self.a_to_v(audio, visual)\n",
    "        a_from_t, _ = self.a_to_t(audio, text)\n",
    "        a_enhanced = audio + a_from_v + a_from_t\n",
    "        \n",
    "        # Combine\n",
    "        combined = torch.cat([v_enhanced, t_enhanced, a_enhanced], dim=-1)\n",
    "        return self.fusion(combined)\n",
    "\n",
    "\n",
    "# Test\n",
    "mm_transformer = MultiModalTransformer(256, 4, 10)\n",
    "output = mm_transformer(v_feat, t_feat, a_feat)\n",
    "print(f\"Multi-Modal Transformer output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Multi-Modal Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalNetwork(nn.Module):\n",
    "    \"\"\"Complete multi-modal classification network.\"\"\"\n",
    "    def __init__(self, hidden_dim=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        # Encoders\n",
    "        self.visual_encoder = VisualEncoder(hidden_dim)\n",
    "        self.text_encoder = TextEncoder(128, hidden_dim)\n",
    "        self.audio_encoder = AudioEncoder(hidden_dim)\n",
    "        \n",
    "        # Cross-modal fusion\n",
    "        self.cross_attention = MultiModalTransformer(hidden_dim, 4, num_classes)\n",
    "    \n",
    "    def forward(self, visual, text, audio):\n",
    "        # Encode each modality\n",
    "        v_feat = self.visual_encoder(visual)\n",
    "        t_feat = self.text_encoder(text)\n",
    "        a_feat = self.audio_encoder(audio)\n",
    "        \n",
    "        # Fuse and classify\n",
    "        return self.cross_attention(v_feat, t_feat, a_feat)\n",
    "\n",
    "\n",
    "# Create and test\n",
    "model = MultiModalNetwork(hidden_dim=256, num_classes=10)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "output = model(visual, text, audio)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Simulate training\n",
    "labels = torch.randint(0, 10, (batch_size,))\n",
    "loss = F.cross_entropy(output, labels)\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Watts-Strogatz Connectivity for Multi-Modal Fusion\n",
    "\n",
    "We can use small-world topology to connect modality-specific modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def create_ws_cross_modal_mask(n_modalities, features_per_modality, k=4, beta=0.3):\n",
    "    \"\"\"\n",
    "    Create a Watts-Strogatz connectivity pattern between modality features.\n",
    "    \"\"\"\n",
    "    total_features = n_modalities * features_per_modality\n",
    "    \n",
    "    # Create WS graph\n",
    "    G = nx.watts_strogatz_graph(total_features, k, beta, seed=42)\n",
    "    adj = nx.to_numpy_array(G)\n",
    "    \n",
    "    return torch.FloatTensor(adj)\n",
    "\n",
    "\n",
    "class WSMultiModalFusion(nn.Module):\n",
    "    \"\"\"Multi-modal fusion using Watts-Strogatz connectivity.\"\"\"\n",
    "    def __init__(self, n_modalities=3, features_per_modality=64, k=4, beta=0.3):\n",
    "        super().__init__()\n",
    "        total = n_modalities * features_per_modality\n",
    "        \n",
    "        # Learnable weights\n",
    "        self.weight = nn.Parameter(torch.randn(total, total) * 0.01)\n",
    "        \n",
    "        # WS connectivity mask\n",
    "        mask = create_ws_cross_modal_mask(n_modalities, features_per_modality, k, beta)\n",
    "        self.register_buffer('mask', mask)\n",
    "        \n",
    "        self.n_modalities = n_modalities\n",
    "        self.features_per_modality = features_per_modality\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # features: list of [batch, features_per_modality] tensors\n",
    "        concat = torch.cat(features, dim=-1)  # [batch, total]\n",
    "        \n",
    "        # Apply sparse WS connectivity\n",
    "        masked_weight = self.weight * self.mask\n",
    "        output = F.linear(concat, masked_weight)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_sparsity(self):\n",
    "        return 1 - (self.mask.sum() / self.mask.numel()).item()\n",
    "\n",
    "\n",
    "# Test\n",
    "ws_fusion = WSMultiModalFusion(n_modalities=3, features_per_modality=64, k=6, beta=0.3)\n",
    "\n",
    "# Create features\n",
    "features = [torch.randn(8, 64) for _ in range(3)]\n",
    "output = ws_fusion(features)\n",
    "\n",
    "print(f\"WS Fusion output shape: {output.shape}\")\n",
    "print(f\"Connectivity sparsity: {ws_fusion.get_sparsity():.1%}\")\n",
    "\n",
    "# Visualize mask\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(ws_fusion.mask.numpy(), cmap='Blues')\n",
    "plt.title('Watts-Strogatz Cross-Modal Connectivity')\n",
    "plt.xlabel('Output Features')\n",
    "plt.ylabel('Input Features')\n",
    "\n",
    "# Add modality boundaries\n",
    "for i in range(1, 3):\n",
    "    pos = i * 64\n",
    "    plt.axhline(pos, color='red', linestyle='--', alpha=0.5)\n",
    "    plt.axvline(pos, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.colorbar(label='Connection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key concepts covered:\n",
    "\n",
    "1. **Modality-Specific Encoders**: CNN for vision/audio, Transformer for text\n",
    "2. **Fusion Strategies**: Early (concatenate), Late (combine predictions), Hybrid\n",
    "3. **Cross-Modal Attention**: Let each modality attend to others\n",
    "4. **Complete Pipeline**: Encoders + Fusion + Classifier\n",
    "5. **WS Topology**: Use small-world connectivity for efficient cross-modal fusion\n",
    "\n",
    "## Why Multi-Modal + Sparse WS?\n",
    "\n",
    "- Multi-modal captures complementary information\n",
    "- WS topology provides efficient information routing\n",
    "- Sparse connections reduce parameters while maintaining performance\n",
    "- Learnable beta can adapt connectivity during training\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [->] Module 10: Capstone - Build Your Own Segmented WS Multi-Modal Architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
