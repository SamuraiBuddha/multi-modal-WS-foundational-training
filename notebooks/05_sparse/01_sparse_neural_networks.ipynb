{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05: Sparse Neural Networks\n",
    "\n",
    "Sparse networks use fewer connections than dense networks while maintaining performance.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand sparsity and its benefits\n",
    "- Learn about pruning techniques\n",
    "- Implement sparse layers with binary masks\n",
    "- Explore the Lottery Ticket Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"[OK] Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Sparsity?\n",
    "\n",
    "Dense networks have many parameters that may not all be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare parameter counts\n",
    "input_dim = 784  # e.g., MNIST\n",
    "hidden_dim = 512\n",
    "output_dim = 10\n",
    "\n",
    "dense_params = input_dim * hidden_dim + hidden_dim * output_dim\n",
    "print(f\"Dense network parameters: {dense_params:,}\")\n",
    "\n",
    "# With 90% sparsity\n",
    "sparsity = 0.9\n",
    "sparse_params = int(dense_params * (1 - sparsity))\n",
    "print(f\"90% sparse network parameters: {sparse_params:,}\")\n",
    "print(f\"Compression ratio: {dense_params / sparse_params:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of Sparsity\n",
    "\n",
    "Sparsity can be unstructured (individual weights) or structured (entire neurons/channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different sparsity patterns\n",
    "def create_sparse_mask(shape, sparsity, pattern='random'):\n",
    "    \"\"\"Create different sparse masks.\"\"\"\n",
    "    if pattern == 'random':\n",
    "        mask = (torch.rand(shape) > sparsity).float()\n",
    "    elif pattern == 'row':  # Structured: remove entire rows\n",
    "        mask = torch.ones(shape)\n",
    "        rows_to_remove = int(shape[0] * sparsity)\n",
    "        mask[:rows_to_remove, :] = 0\n",
    "    elif pattern == 'block':  # Block sparsity\n",
    "        block_size = 4\n",
    "        mask = torch.ones(shape)\n",
    "        for i in range(0, shape[0], block_size):\n",
    "            for j in range(0, shape[1], block_size):\n",
    "                if torch.rand(1) < sparsity:\n",
    "                    mask[i:i+block_size, j:j+block_size] = 0\n",
    "    return mask\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "patterns = ['random', 'row', 'block']\n",
    "titles = ['Unstructured (Random)', 'Structured (Row)', 'Block Sparsity']\n",
    "\n",
    "for ax, pattern, title in zip(axes, patterns, titles):\n",
    "    mask = create_sparse_mask((32, 32), 0.7, pattern)\n",
    "    ax.imshow(mask, cmap='Blues')\n",
    "    ax.set_title(f\"{title}\\n({(mask==0).sum().item()}/{mask.numel()} zeros)\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pruning Methods\n",
    "\n",
    "Pruning removes connections from a trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_prune(weights, sparsity):\n",
    "    \"\"\"Prune weights with smallest magnitude.\"\"\"\n",
    "    threshold = torch.quantile(weights.abs().flatten(), sparsity)\n",
    "    mask = (weights.abs() > threshold).float()\n",
    "    return mask\n",
    "\n",
    "def random_prune(weights, sparsity):\n",
    "    \"\"\"Random pruning (baseline).\"\"\"\n",
    "    mask = (torch.rand_like(weights) > sparsity).float()\n",
    "    return mask\n",
    "\n",
    "# Compare pruning methods\n",
    "weights = torch.randn(100, 100)\n",
    "\n",
    "mag_mask = magnitude_prune(weights, 0.8)\n",
    "rand_mask = random_prune(weights, 0.8)\n",
    "\n",
    "# Check which preserves larger weights\n",
    "mag_preserved = (weights.abs() * mag_mask).sum()\n",
    "rand_preserved = (weights.abs() * rand_mask).sum()\n",
    "\n",
    "print(f\"Magnitude pruning preserves: {mag_preserved:.2f} total magnitude\")\n",
    "print(f\"Random pruning preserves: {rand_preserved:.2f} total magnitude\")\n",
    "print(f\"[OK] Magnitude pruning keeps {mag_preserved/rand_preserved:.2f}x more important weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Sparse Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseLinear(nn.Module):\n",
    "    \"\"\"Linear layer with sparse connectivity.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, density=0.3, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Weights\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.01)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        # Binary mask (not a parameter - not trained)\n",
    "        mask = (torch.rand(out_features, in_features) < density).float()\n",
    "        self.register_buffer('mask', mask)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply mask to weights\n",
    "        masked_weight = self.weight * self.mask\n",
    "        return F.linear(x, masked_weight, self.bias)\n",
    "    \n",
    "    def get_sparsity(self):\n",
    "        \"\"\"Return current sparsity level.\"\"\"\n",
    "        total = self.mask.numel()\n",
    "        zeros = (self.mask == 0).sum().item()\n",
    "        return zeros / total\n",
    "\n",
    "# Test\n",
    "layer = SparseLinear(100, 50, density=0.2)\n",
    "x = torch.randn(8, 100)\n",
    "y = layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Sparsity: {layer.get_sparsity():.1%}\")\n",
    "print(f\"Active connections: {(layer.mask == 1).sum().item()} / {layer.mask.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Sparse Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, density=0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = SparseLinear(input_dim, hidden_dim, density=density)\n",
    "        self.fc2 = SparseLinear(hidden_dim, output_dim, density=density)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Create synthetic data\n",
    "X = torch.randn(500, 20)\n",
    "y = (X[:, 0] + X[:, 1] > 0).long()  # Simple classification\n",
    "\n",
    "# Train sparse vs dense\n",
    "def train_model(model, X, y, epochs=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    # Accuracy\n",
    "    with torch.no_grad():\n",
    "        pred = model(X).argmax(1)\n",
    "        acc = (pred == y).float().mean().item()\n",
    "    \n",
    "    return losses, acc\n",
    "\n",
    "# Compare different sparsity levels\n",
    "densities = [1.0, 0.5, 0.3, 0.1]\n",
    "results = {}\n",
    "\n",
    "for d in densities:\n",
    "    torch.manual_seed(42)\n",
    "    if d == 1.0:\n",
    "        model = nn.Sequential(nn.Linear(20, 64), nn.ReLU(), nn.Linear(64, 2))\n",
    "    else:\n",
    "        model = SparseMLP(20, 64, 2, density=d)\n",
    "    \n",
    "    losses, acc = train_model(model, X, y)\n",
    "    results[d] = (losses, acc)\n",
    "    print(f\"Density {d:.0%}: Final accuracy = {acc:.1%}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "for d, (losses, acc) in results.items():\n",
    "    label = f\"{'Dense' if d == 1.0 else f'{(1-d):.0%} Sparse'} (acc={acc:.1%})\"\n",
    "    plt.plot(losses, label=label)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Sparse vs Dense Training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Lottery Ticket Hypothesis\n",
    "\n",
    "Dense networks contain sparse subnetworks (\"winning tickets\") that can train to the same accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Lottery Ticket experiment\n",
    "def lottery_ticket_experiment(X, y, target_sparsity=0.9, iterations=3):\n",
    "    \"\"\"Iterative magnitude pruning to find winning tickets.\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Initial weights (the \"lottery ticket\")\n",
    "    init_weights = {\n",
    "        'fc1.weight': torch.randn(64, 20) * 0.1,\n",
    "        'fc1.bias': torch.zeros(64),\n",
    "        'fc2.weight': torch.randn(2, 64) * 0.1,\n",
    "        'fc2.bias': torch.zeros(2),\n",
    "    }\n",
    "    \n",
    "    # Create masks (start fully connected)\n",
    "    masks = {\n",
    "        'fc1.weight': torch.ones(64, 20),\n",
    "        'fc2.weight': torch.ones(2, 64),\n",
    "    }\n",
    "    \n",
    "    prune_rate = 1 - (1 - target_sparsity) ** (1 / iterations)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(iterations + 1):\n",
    "        # Create model with current mask and initial weights\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(20, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "        # Load initial weights\n",
    "        model[0].weight.data = init_weights['fc1.weight'].clone() * masks['fc1.weight']\n",
    "        model[0].bias.data = init_weights['fc1.bias'].clone()\n",
    "        model[2].weight.data = init_weights['fc2.weight'].clone() * masks['fc2.weight']\n",
    "        model[2].bias.data = init_weights['fc2.bias'].clone()\n",
    "        \n",
    "        # Train\n",
    "        _, acc = train_model(model, X, y, epochs=100)\n",
    "        \n",
    "        current_sparsity = 1 - (masks['fc1.weight'].sum() + masks['fc2.weight'].sum()) / \\\n",
    "                          (masks['fc1.weight'].numel() + masks['fc2.weight'].numel())\n",
    "        \n",
    "        results.append((current_sparsity.item(), acc))\n",
    "        print(f\"Iteration {i}: Sparsity={current_sparsity:.1%}, Accuracy={acc:.1%}\")\n",
    "        \n",
    "        # Prune for next iteration\n",
    "        if i < iterations:\n",
    "            for name in ['fc1.weight', 'fc2.weight']:\n",
    "                layer_idx = 0 if 'fc1' in name else 2\n",
    "                weights = model[layer_idx].weight.data.abs()\n",
    "                threshold = torch.quantile(weights[masks[name] == 1], prune_rate)\n",
    "                masks[name] = ((weights > threshold) & (masks[name] == 1)).float()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Lottery Ticket Hypothesis Demonstration:\")\n",
    "print(\"-\" * 50)\n",
    "results = lottery_ticket_experiment(X, y, target_sparsity=0.8, iterations=3)\n",
    "\n",
    "print(\"\\n[OK] Finding: Sparse subnetworks can match dense performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key concepts covered:\n",
    "\n",
    "1. **Sparsity Benefits**: Fewer parameters, less compute, potential regularization\n",
    "2. **Types**: Unstructured (random), structured (rows/channels), block\n",
    "3. **Pruning**: Magnitude-based pruning keeps important weights\n",
    "4. **Sparse Layers**: Binary masks multiply weights to enforce sparsity\n",
    "5. **Lottery Ticket**: Dense nets contain trainable sparse subnetworks\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [->] Module 06: Unsupervised Learning\n",
    "- [->] Module 07: Dynamic Sparse Training (SET, DEEP R)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
