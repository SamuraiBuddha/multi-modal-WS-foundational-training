{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10: Capstone - Build the Segmented WS Multi-Modal Architecture\n",
    "\n",
    "Congratulations on reaching the capstone! In this notebook, you'll build the complete architecture that combines everything you've learned:\n",
    "\n",
    "- Modality-specific encoders (visual, text, audio)\n",
    "- Watts-Strogatz inter-module connector\n",
    "- Learnable beta parameter\n",
    "- Multi-modal fusion\n",
    "- Dynamic sparse training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.models import (\n",
    "    SegmentedWSArchitecture,\n",
    "    MultiModalWSNetwork,\n",
    ")\n",
    "from src.data import SyntheticMultiModal, create_dataloaders\n",
    "from src.training import Trainer, TrainingConfig, SparseTrainer\n",
    "from src.visualization import (\n",
    "    plot_loss_curves,\n",
    "    create_training_dashboard,\n",
    "    plot_inter_module_connectivity,\n",
    ")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 The Architecture Overview\n",
    "\n",
    "```\n",
    "+-------------------------------------------------------------------+\n",
    "|  +----------+   +----------+   +----------+                       |\n",
    "|  |  Visual  |   |   Text   |   |  Audio   |   <- Modality         |\n",
    "|  |  Module  |   |  Module  |   |  Module  |      Encoders         |\n",
    "|  +----+-----+   +----+-----+   +----+-----+                       |\n",
    "|       |              |              |                              |\n",
    "|       +-------+------+------+-------+                              |\n",
    "|               |                                                    |\n",
    "|        +------v------+                                             |\n",
    "|        |   WS Inter- |  <- Learnable Small-World                  |\n",
    "|        |   Module    |     Connector with Dynamic                  |\n",
    "|        |   Connector |     Rewiring (beta learnable)               |\n",
    "|        +------+------+                                             |\n",
    "|               |                                                    |\n",
    "|        +------v------+                                             |\n",
    "|        |    Fusion   |  <- Cross-Modal Integration                 |\n",
    "|        |    Module   |                                             |\n",
    "|        +-------------+                                             |\n",
    "+-------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Create the Dataset\n",
    "\n",
    "We'll use synthetic multi-modal data for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic multi-modal dataset\n",
    "train_dataset = SyntheticMultiModal(\n",
    "    n_samples=5000,\n",
    "    visual_dim=(1, 28, 28),\n",
    "    text_seq_len=32,\n",
    "    vocab_size=1000,\n",
    "    audio_dim=(128, 64),\n",
    "    n_classes=10,\n",
    "    correlation=0.7,  # How correlated modalities are with labels\n",
    ")\n",
    "\n",
    "val_dataset = SyntheticMultiModal(\n",
    "    n_samples=1000,\n",
    "    visual_dim=(1, 28, 28),\n",
    "    text_seq_len=32,\n",
    "    vocab_size=1000,\n",
    "    audio_dim=(128, 64),\n",
    "    n_classes=10,\n",
    "    correlation=0.7,\n",
    "    seed=123,  # Different seed for validation\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Check a sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample keys: {sample.keys()}\")\n",
    "print(f\"Visual shape: {sample['visual'].shape}\")\n",
    "print(f\"Text shape: {sample['text'].shape}\")\n",
    "print(f\"Audio shape: {sample['audio'].shape}\")\n",
    "print(f\"Label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Build the Architecture\n",
    "\n",
    "Now let's create our Segmented Watts-Strogatz Multi-Modal Architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the architecture\n",
    "model = SegmentedWSArchitecture(\n",
    "    visual_config={'input_shape': (1, 28, 28), 'hidden_dims': [256, 128]},\n",
    "    text_config={'vocab_size': 1000, 'embed_dim': 64, 'hidden_dim': 128},\n",
    "    audio_config={'input_dim': 128, 'hidden_dims': [256, 128]},\n",
    "    segment_dim=64,         # Dimension of each module's output\n",
    "    n_ws_layers=2,          # WS-connected processing layers\n",
    "    ws_k=2,                 # Initial neighbors in WS topology\n",
    "    initial_beta=0.3,       # Starting rewiring probability\n",
    "    use_moe=False,          # Use Mixture of Experts (try True!)\n",
    "    sparse_layers=True,     # Use sparse connectivity\n",
    "    layer_density=0.3,      # 30% density (70% sparse)\n",
    "    output_dim=10,          # 10 classes\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Print architecture stats\n",
    "stats = model.get_architecture_stats()\n",
    "print(\"Architecture Statistics:\")\n",
    "print(f\"  Total parameters: {stats['total_params']:,}\")\n",
    "print(f\"  Trainable parameters: {stats['trainable_params']:,}\")\n",
    "print(f\"  WS layers: {stats['n_ws_layers']}\")\n",
    "print(f\"  Beta values: {stats['betas']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "batch = train_dataset[0]\n",
    "\n",
    "# Add batch dimension and move to device\n",
    "visual = batch['visual'].unsqueeze(0).to(device)\n",
    "text = batch['text'].unsqueeze(0).to(device)\n",
    "audio = batch['audio'].unsqueeze(0).to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(visual=visual, text=text, audio=audio)\n",
    "\n",
    "print(f\"Input shapes:\")\n",
    "print(f\"  Visual: {visual.shape}\")\n",
    "print(f\"  Text: {text.shape}\")\n",
    "print(f\"  Audio: {audio.shape}\")\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Predictions: {torch.softmax(output, dim=-1).squeeze()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Training the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create data loaders\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for multi-modal data.\"\"\"\n",
    "    return {\n",
    "        'visual': torch.stack([b['visual'] for b in batch]),\n",
    "        'text': torch.stack([b['text'] for b in batch]),\n",
    "        'audio': torch.stack([b['audio'] for b in batch]),\n",
    "        'label': torch.stack([b['label'] for b in batch]),\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "n_epochs = 10  # Use more epochs for better results\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        visual = batch['visual'].to(device)\n",
    "        text = batch['text'].to(device)\n",
    "        audio = batch['audio'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(visual=visual, text=text, audio=audio)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Add auxiliary loss if using MoE\n",
    "        if hasattr(model, 'aux_loss'):\n",
    "            loss = loss + model.aux_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            visual = batch['visual'].to(device)\n",
    "            text = batch['text'].to(device)\n",
    "            audio = batch['audio'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(visual=visual, text=text, audio=audio)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_loss / len(train_loader))\n",
    "    history['val_loss'].append(val_loss / len(val_loader))\n",
    "    history['train_acc'].append(100 * train_correct / train_total)\n",
    "    history['val_acc'].append(100 * val_correct / val_total)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}: \"\n",
    "          f\"Train Loss={history['train_loss'][-1]:.4f}, \"\n",
    "          f\"Train Acc={history['train_acc'][-1]:.1f}%, \"\n",
    "          f\"Val Acc={history['val_acc'][-1]:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig = plot_loss_curves(history, title='Segmented WS Architecture Training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Analyze the Learned Architecture\n",
    "\n",
    "Let's examine what the model learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check learned beta values\n",
    "print(\"Learned WS parameters:\")\n",
    "print(f\"  Beta values per layer: {model.betas}\")\n",
    "\n",
    "# Visualize inter-module connectivity\n",
    "adj = model.segment_adj.cpu().numpy()\n",
    "fig = plot_inter_module_connectivity(\n",
    "    adj, \n",
    "    module_names=['Visual', 'Text', 'Audio']\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 Exercise: Experiment with the Architecture\n",
    "\n",
    "Try modifying the architecture:\n",
    "\n",
    "1. Enable Mixture of Experts (`use_moe=True`)\n",
    "2. Change the sparsity level (`layer_density`)\n",
    "3. Adjust the initial beta value\n",
    "4. Try different segment dimensions\n",
    "\n",
    "What gives the best accuracy vs parameter efficiency tradeoff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR EXPERIMENTS HERE\n",
    "# Example: Try with MoE enabled\n",
    "# model_moe = SegmentedWSArchitecture(\n",
    "#     ...\n",
    "#     use_moe=True,\n",
    "#     n_experts=4,\n",
    "#     ...\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've built a complete Segmented Watts-Strogatz Multi-Modal Architecture!\n",
    "\n",
    "**Key innovations in this architecture:**\n",
    "\n",
    "1. **Heterogeneous modules** - Different encoder architectures for each modality\n",
    "2. **WS topology** - Small-world connectivity for efficient inter-module communication\n",
    "3. **Learnable beta** - The network can optimize its own topology\n",
    "4. **Sparse connectivity** - Fewer parameters with maintained performance\n",
    "5. **Optional MoE** - Expert routing for modality-specific processing\n",
    "\n",
    "**What you've learned in this curriculum:**\n",
    "\n",
    "- Neural network fundamentals\n",
    "- Graph theory and network topology\n",
    "- Sparse neural networks\n",
    "- Dynamic sparse training (SET, DEEP R)\n",
    "- Multi-modal learning\n",
    "- How to combine all these into a novel architecture!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
