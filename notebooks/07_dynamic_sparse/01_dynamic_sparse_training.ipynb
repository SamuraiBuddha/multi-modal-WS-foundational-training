{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 07: Dynamic Sparse Training\n",
    "\n",
    "Dynamic sparse training evolves network connectivity during training, not just at the end.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand SET (Sparse Evolutionary Training)\n",
    "- Learn DEEP R (Deep Rewiring) algorithm\n",
    "- Implement dynamic rewiring in PyTorch\n",
    "- Visualize topology evolution during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"[OK] Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Static vs Dynamic Sparsity\n",
    "\n",
    "Static sparsity fixes the mask; dynamic sparsity evolves it during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Static sparse\n",
    "static_mask = (torch.rand(20, 20) > 0.7).float()\n",
    "axes[0].imshow(static_mask, cmap='Blues')\n",
    "axes[0].set_title('Static Sparsity\\n(fixed mask)')\n",
    "\n",
    "# Dynamic sparse - evolves\n",
    "dynamic_mask = static_mask.clone()\n",
    "# Simulate some rewiring\n",
    "for _ in range(5):\n",
    "    # Remove some connections\n",
    "    active = torch.where(dynamic_mask == 1)\n",
    "    n_remove = len(active[0]) // 10\n",
    "    indices_to_remove = torch.randperm(len(active[0]))[:n_remove]\n",
    "    for idx in indices_to_remove:\n",
    "        dynamic_mask[active[0][idx], active[1][idx]] = 0\n",
    "    \n",
    "    # Add new connections\n",
    "    inactive = torch.where(dynamic_mask == 0)\n",
    "    indices_to_add = torch.randperm(len(inactive[0]))[:n_remove]\n",
    "    for idx in indices_to_add:\n",
    "        dynamic_mask[inactive[0][idx], inactive[1][idx]] = 1\n",
    "\n",
    "axes[1].imshow(dynamic_mask, cmap='Blues')\n",
    "axes[1].set_title('Dynamic Sparsity\\n(evolved mask)')\n",
    "\n",
    "# Show difference\n",
    "diff = dynamic_mask - static_mask\n",
    "axes[2].imshow(diff, cmap='RdBu', vmin=-1, vmax=1)\n",
    "axes[2].set_title('Difference\\n(red=removed, blue=added)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Connections changed: {(diff != 0).sum().item()} / {diff.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SET Algorithm\n",
    "\n",
    "Sparse Evolutionary Training (SET) prunes weak connections and regrows random new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_rewire(weights, mask, prune_rate=0.3, regrow_rate=0.3):\n",
    "    \"\"\"\n",
    "    SET (Sparse Evolutionary Training) rewiring.\n",
    "    \n",
    "    1. Prune: Remove connections with smallest magnitude\n",
    "    2. Regrow: Add new random connections\n",
    "    \"\"\"\n",
    "    # Only consider active connections\n",
    "    active_weights = weights * mask\n",
    "    active_indices = torch.where(mask == 1)\n",
    "    \n",
    "    if len(active_indices[0]) == 0:\n",
    "        return mask\n",
    "    \n",
    "    # Get magnitudes of active connections\n",
    "    magnitudes = active_weights[active_indices].abs()\n",
    "    \n",
    "    # Prune smallest magnitude connections\n",
    "    n_prune = int(len(magnitudes) * prune_rate)\n",
    "    if n_prune > 0:\n",
    "        threshold = torch.kthvalue(magnitudes, n_prune).values\n",
    "        prune_mask = magnitudes <= threshold\n",
    "        \n",
    "        # Create new mask\n",
    "        new_mask = mask.clone()\n",
    "        for i, idx in enumerate(zip(active_indices[0], active_indices[1])):\n",
    "            if prune_mask[i]:\n",
    "                new_mask[idx[0], idx[1]] = 0\n",
    "    else:\n",
    "        new_mask = mask.clone()\n",
    "    \n",
    "    # Regrow: add random new connections\n",
    "    inactive = torch.where(new_mask == 0)\n",
    "    n_regrow = int(len(inactive[0]) * regrow_rate * (n_prune / max(1, len(inactive[0]))))\n",
    "    n_regrow = min(n_regrow, n_prune, len(inactive[0]))\n",
    "    \n",
    "    if n_regrow > 0:\n",
    "        regrow_indices = torch.randperm(len(inactive[0]))[:n_regrow]\n",
    "        for idx in regrow_indices:\n",
    "            new_mask[inactive[0][idx], inactive[1][idx]] = 1\n",
    "    \n",
    "    return new_mask\n",
    "\n",
    "# Demo\n",
    "weights = torch.randn(10, 10)\n",
    "mask = (torch.rand(10, 10) > 0.7).float()\n",
    "\n",
    "print(f\"Before: {mask.sum().item():.0f} active connections\")\n",
    "new_mask = set_rewire(weights, mask, prune_rate=0.3, regrow_rate=0.3)\n",
    "print(f\"After: {new_mask.sum().item():.0f} active connections\")\n",
    "print(f\"[OK] SET maintains similar sparsity while evolving topology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DEEP R Algorithm\n",
    "\n",
    "DEEP R uses gradient information to decide which connections to regrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_r_rewire(weights, mask, gradients, temperature=1.0, prune_rate=0.3):\n",
    "    \"\"\"\n",
    "    DEEP R (Deep Rewiring) algorithm.\n",
    "    \n",
    "    1. Prune: Remove connections with smallest magnitude\n",
    "    2. Regrow: Add connections where gradients are large (gradient-guided)\n",
    "    \"\"\"\n",
    "    # Prune weak connections (same as SET)\n",
    "    active_weights = weights * mask\n",
    "    active_indices = torch.where(mask == 1)\n",
    "    \n",
    "    if len(active_indices[0]) == 0:\n",
    "        return mask\n",
    "    \n",
    "    magnitudes = active_weights[active_indices].abs()\n",
    "    n_prune = int(len(magnitudes) * prune_rate)\n",
    "    \n",
    "    if n_prune > 0:\n",
    "        threshold = torch.kthvalue(magnitudes, n_prune).values\n",
    "        prune_mask = magnitudes <= threshold\n",
    "        \n",
    "        new_mask = mask.clone()\n",
    "        for i, idx in enumerate(zip(active_indices[0], active_indices[1])):\n",
    "            if prune_mask[i]:\n",
    "                new_mask[idx[0], idx[1]] = 0\n",
    "    else:\n",
    "        new_mask = mask.clone()\n",
    "    \n",
    "    # DEEP R: Gradient-guided regrowth\n",
    "    inactive = torch.where(new_mask == 0)\n",
    "    if len(inactive[0]) > 0 and n_prune > 0:\n",
    "        # Use gradient magnitude for inactive connections\n",
    "        inactive_grads = gradients[inactive].abs()\n",
    "        \n",
    "        # Softmax with temperature for probabilistic selection\n",
    "        probs = F.softmax(inactive_grads / temperature, dim=0)\n",
    "        \n",
    "        # Sample connections to regrow based on gradient magnitude\n",
    "        n_regrow = min(n_prune, len(inactive[0]))\n",
    "        regrow_indices = torch.multinomial(probs, n_regrow, replacement=False)\n",
    "        \n",
    "        for idx in regrow_indices:\n",
    "            new_mask[inactive[0][idx], inactive[1][idx]] = 1\n",
    "    \n",
    "    return new_mask\n",
    "\n",
    "# Demo with synthetic gradients\n",
    "weights = torch.randn(10, 10)\n",
    "mask = (torch.rand(10, 10) > 0.7).float()\n",
    "gradients = torch.randn(10, 10)  # Simulated gradients\n",
    "\n",
    "print(f\"Before: {mask.sum().item():.0f} active connections\")\n",
    "new_mask = deep_r_rewire(weights, mask, gradients, temperature=1.0, prune_rate=0.3)\n",
    "print(f\"After: {new_mask.sum().item():.0f} active connections\")\n",
    "print(f\"[OK] DEEP R uses gradients to guide where to add connections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dynamic Sparse Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicSparseLinear(nn.Module):\n",
    "    \"\"\"Linear layer with dynamic sparse training support.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, density=0.3):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.01)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        \n",
    "        mask = (torch.rand(out_features, in_features) < density).float()\n",
    "        self.register_buffer('mask', mask)\n",
    "        self.register_buffer('_stored_grad', torch.zeros_like(self.weight))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight * self.mask, self.bias)\n",
    "    \n",
    "    def store_gradients(self):\n",
    "        \"\"\"Store gradients for DEEP R.\"\"\"\n",
    "        if self.weight.grad is not None:\n",
    "            self._stored_grad = self.weight.grad.clone()\n",
    "    \n",
    "    def rewire(self, method='set', prune_rate=0.3, temperature=1.0):\n",
    "        \"\"\"Rewire the layer connectivity.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            if method == 'set':\n",
    "                self.mask.data = set_rewire(\n",
    "                    self.weight.data, self.mask, prune_rate=prune_rate\n",
    "                )\n",
    "            elif method == 'deep_r':\n",
    "                self.mask.data = deep_r_rewire(\n",
    "                    self.weight.data, self.mask, self._stored_grad,\n",
    "                    temperature=temperature, prune_rate=prune_rate\n",
    "                )\n",
    "    \n",
    "    def get_sparsity(self):\n",
    "        return 1 - (self.mask.sum() / self.mask.numel()).item()\n",
    "\n",
    "# Test\n",
    "layer = DynamicSparseLinear(100, 50, density=0.2)\n",
    "x = torch.randn(8, 100)\n",
    "y = layer(x)\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Initial sparsity: {layer.get_sparsity():.1%}\")\n",
    "layer.store_gradients()\n",
    "layer.rewire(method='set', prune_rate=0.2)\n",
    "print(f\"After SET rewire: {layer.get_sparsity():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training with Dynamic Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicSparseMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, density=0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = DynamicSparseLinear(input_dim, hidden_dim, density)\n",
    "        self.fc2 = DynamicSparseLinear(hidden_dim, output_dim, density)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def rewire_all(self, method='set', prune_rate=0.3, temperature=1.0):\n",
    "        for layer in [self.fc1, self.fc2]:\n",
    "            layer.store_gradients()\n",
    "            layer.rewire(method, prune_rate, temperature)\n",
    "\n",
    "# Create data\n",
    "X = torch.randn(500, 20)\n",
    "y = (X[:, 0] + X[:, 1] > 0).long()\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train with dynamic sparsity\n",
    "model = DynamicSparseMLP(20, 64, 2, density=0.3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "sparsities = []\n",
    "rewire_frequency = 50  # Rewire every 50 steps\n",
    "step = 0\n",
    "\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch_x)\n",
    "        loss = criterion(out, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        step += 1\n",
    "        if step % rewire_frequency == 0:\n",
    "            model.rewire_all(method='set', prune_rate=0.2)\n",
    "    \n",
    "    losses.append(epoch_loss / len(loader))\n",
    "    avg_sparsity = (model.fc1.get_sparsity() + model.fc2.get_sparsity()) / 2\n",
    "    sparsities.append(avg_sparsity)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss={losses[-1]:.4f}, Sparsity={avg_sparsity:.1%}\")\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(losses)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "\n",
    "ax2.plot([s * 100 for s in sparsities])\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Sparsity (%)')\n",
    "ax2.set_title('Network Sparsity During Training')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing Topology Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track mask changes over time\n",
    "torch.manual_seed(42)\n",
    "layer = DynamicSparseLinear(16, 16, density=0.3)\n",
    "masks_history = [layer.mask.clone()]\n",
    "\n",
    "# Simulate training with rewiring\n",
    "for i in range(10):\n",
    "    # Fake forward/backward\n",
    "    x = torch.randn(4, 16)\n",
    "    y = layer(x).sum()\n",
    "    y.backward()\n",
    "    \n",
    "    layer.store_gradients()\n",
    "    layer.rewire(method='set', prune_rate=0.3)\n",
    "    masks_history.append(layer.mask.clone())\n",
    "    layer.weight.grad = None\n",
    "\n",
    "# Visualize evolution\n",
    "fig, axes = plt.subplots(2, 6, figsize=(15, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (ax, mask) in enumerate(zip(axes, masks_history[::1])):\n",
    "    ax.imshow(mask, cmap='Blues')\n",
    "    ax.set_title(f'Step {i}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Topology Evolution with SET Rewiring', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Measure how much changed\n",
    "total_changed = 0\n",
    "for i in range(1, len(masks_history)):\n",
    "    changed = (masks_history[i] != masks_history[i-1]).sum().item()\n",
    "    total_changed += changed\n",
    "\n",
    "print(f\"Total connections changed: {total_changed}\")\n",
    "print(f\"Average per step: {total_changed / (len(masks_history)-1):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key concepts covered:\n",
    "\n",
    "1. **Static vs Dynamic**: Dynamic sparsity evolves connectivity during training\n",
    "2. **SET Algorithm**: Prune weak connections, regrow random new ones\n",
    "3. **DEEP R Algorithm**: Gradient-guided regrowth for smarter topology evolution\n",
    "4. **Implementation**: Store gradients, apply rewiring periodically\n",
    "5. **Visualization**: Watch topology adapt as training progresses\n",
    "\n",
    "## Benefits of Dynamic Sparse Training\n",
    "\n",
    "- Explores more topologies than static sparsity\n",
    "- Can escape local minima by changing connectivity\n",
    "- Gradient-guided methods (DEEP R) find useful connections faster\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [->] Module 08: Mixture of Experts\n",
    "- [->] Module 09: Multi-Modal Architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
