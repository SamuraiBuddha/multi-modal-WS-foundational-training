{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 08: Mixture of Experts\n",
    "\n",
    "Mixture of Experts (MoE) allows models to scale capacity without proportionally increasing compute.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the MoE architecture\n",
    "- Learn about routing mechanisms (top-k, soft)\n",
    "- Implement load balancing\n",
    "- Explore capacity factors and expert utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"[OK] Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MoE Concept\n",
    "\n",
    "Instead of one large network, use multiple specialized \"experts\" and a router to select which experts process each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MoE concept\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Dense model\n",
    "ax1.add_patch(plt.Rectangle((0.3, 0.1), 0.4, 0.8, fill=True, color='lightblue', edgecolor='black'))\n",
    "ax1.text(0.5, 0.5, 'Dense\\nNetwork\\n(all params\\nused)', ha='center', va='center', fontsize=12)\n",
    "ax1.arrow(0.1, 0.5, 0.15, 0, head_width=0.05, head_length=0.02, fc='black')\n",
    "ax1.arrow(0.75, 0.5, 0.15, 0, head_width=0.05, head_length=0.02, fc='black')\n",
    "ax1.text(0.05, 0.5, 'Input', ha='center', va='center')\n",
    "ax1.text(0.95, 0.5, 'Output', ha='center', va='center')\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_title('Dense Network', fontsize=14)\n",
    "ax1.axis('off')\n",
    "\n",
    "# MoE model\n",
    "ax2.add_patch(plt.Rectangle((0.35, 0.4), 0.15, 0.2, fill=True, color='lightgreen', edgecolor='black'))\n",
    "ax2.text(0.425, 0.5, 'Router', ha='center', va='center', fontsize=10)\n",
    "\n",
    "colors = ['lightblue', 'lightcoral', 'lightyellow', 'lightgray']\n",
    "for i, (y, c) in enumerate(zip([0.8, 0.55, 0.3, 0.05], colors)):\n",
    "    alpha = 1.0 if i in [0, 2] else 0.3  # Highlight selected experts\n",
    "    ax2.add_patch(plt.Rectangle((0.55, y), 0.15, 0.15, fill=True, color=c, \n",
    "                                 edgecolor='black', alpha=alpha))\n",
    "    ax2.text(0.625, y + 0.075, f'E{i+1}', ha='center', va='center', fontsize=10)\n",
    "\n",
    "ax2.arrow(0.1, 0.5, 0.2, 0, head_width=0.03, head_length=0.02, fc='black')\n",
    "ax2.arrow(0.75, 0.5, 0.15, 0, head_width=0.03, head_length=0.02, fc='black')\n",
    "ax2.text(0.05, 0.5, 'Input', ha='center', va='center')\n",
    "ax2.text(0.95, 0.5, 'Output', ha='center', va='center')\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_title('Mixture of Experts (only 2 experts active)', fontsize=14)\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"[OK] MoE uses a subset of experts per input --> efficient scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Expert and Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    \"\"\"A single expert network.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Router(nn.Module):\n",
    "    \"\"\"Routes inputs to experts.\"\"\"\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Returns logits for each expert\n",
    "        return self.gate(x)\n",
    "\n",
    "\n",
    "# Test\n",
    "input_dim, hidden_dim, output_dim = 64, 128, 64\n",
    "num_experts = 4\n",
    "\n",
    "experts = nn.ModuleList([Expert(input_dim, hidden_dim, output_dim) for _ in range(num_experts)])\n",
    "router = Router(input_dim, num_experts)\n",
    "\n",
    "x = torch.randn(8, input_dim)\n",
    "routing_logits = router(x)\n",
    "routing_probs = F.softmax(routing_logits, dim=-1)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Routing probabilities shape: {routing_probs.shape}\")\n",
    "print(f\"Sample routing probs: {routing_probs[0].detach().numpy().round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Top-K Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_routing(routing_logits, k=2):\n",
    "    \"\"\"\n",
    "    Select top-k experts for each input.\n",
    "    \n",
    "    Returns:\n",
    "        indices: Which experts to use (batch_size, k)\n",
    "        weights: How much to weight each expert (batch_size, k)\n",
    "    \"\"\"\n",
    "    # Get top-k experts\n",
    "    top_k_logits, top_k_indices = torch.topk(routing_logits, k, dim=-1)\n",
    "    \n",
    "    # Normalize weights among selected experts\n",
    "    top_k_weights = F.softmax(top_k_logits, dim=-1)\n",
    "    \n",
    "    return top_k_indices, top_k_weights\n",
    "\n",
    "# Demo\n",
    "indices, weights = top_k_routing(routing_logits, k=2)\n",
    "\n",
    "print(\"Top-2 Routing:\")\n",
    "for i in range(min(4, len(x))):\n",
    "    print(f\"  Sample {i}: Experts {indices[i].tolist()} with weights {weights[i].detach().numpy().round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete MoE Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureOfExperts(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Experts layer with top-k routing.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_experts=4, top_k=2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(input_dim, hidden_dim, output_dim) \n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # Router\n",
    "        self.router = Router(input_dim, num_experts)\n",
    "        \n",
    "        # For tracking expert utilization\n",
    "        self.register_buffer('expert_counts', torch.zeros(num_experts))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Get routing decisions\n",
    "        routing_logits = self.router(x)\n",
    "        top_k_indices, top_k_weights = top_k_routing(routing_logits, self.top_k)\n",
    "        \n",
    "        # Track expert usage\n",
    "        for idx in top_k_indices.flatten():\n",
    "            self.expert_counts[idx] += 1\n",
    "        \n",
    "        # Compute weighted sum of expert outputs\n",
    "        output = torch.zeros(batch_size, self.experts[0].net[-1].out_features, device=x.device)\n",
    "        \n",
    "        for i in range(self.top_k):\n",
    "            expert_indices = top_k_indices[:, i]\n",
    "            expert_weights = top_k_weights[:, i].unsqueeze(-1)\n",
    "            \n",
    "            for expert_idx in range(self.num_experts):\n",
    "                mask = (expert_indices == expert_idx)\n",
    "                if mask.any():\n",
    "                    expert_input = x[mask]\n",
    "                    expert_output = self.experts[expert_idx](expert_input)\n",
    "                    output[mask] += expert_weights[mask] * expert_output\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_expert_utilization(self):\n",
    "        \"\"\"Return normalized expert utilization.\"\"\"\n",
    "        total = self.expert_counts.sum()\n",
    "        if total > 0:\n",
    "            return self.expert_counts / total\n",
    "        return self.expert_counts\n",
    "    \n",
    "    def reset_counts(self):\n",
    "        self.expert_counts.zero_()\n",
    "\n",
    "# Test\n",
    "moe = MixtureOfExperts(64, 128, 64, num_experts=4, top_k=2)\n",
    "x = torch.randn(32, 64)\n",
    "output = moe(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expert utilization: {moe.get_expert_utilization().numpy().round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Balancing Loss\n",
    "\n",
    "Without load balancing, the router may always select the same experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balancing_loss(routing_logits, num_experts):\n",
    "    \"\"\"\n",
    "    Compute load balancing loss to encourage even expert utilization.\n",
    "    \n",
    "    Based on Switch Transformer paper.\n",
    "    \"\"\"\n",
    "    # Router probabilities\n",
    "    probs = F.softmax(routing_logits, dim=-1)  # (batch, num_experts)\n",
    "    \n",
    "    # Fraction of tokens routed to each expert\n",
    "    tokens_per_expert = probs.mean(dim=0)  # Average over batch\n",
    "    \n",
    "    # We want uniform distribution = 1/num_experts per expert\n",
    "    # Auxiliary loss encourages this\n",
    "    aux_loss = num_experts * (tokens_per_expert * tokens_per_expert).sum()\n",
    "    \n",
    "    return aux_loss\n",
    "\n",
    "# Demo: Compare balanced vs unbalanced routing\n",
    "# Unbalanced: router always prefers expert 0\n",
    "unbalanced_logits = torch.zeros(32, 4)\n",
    "unbalanced_logits[:, 0] = 10  # Strong preference for expert 0\n",
    "\n",
    "# Balanced: uniform distribution\n",
    "balanced_logits = torch.zeros(32, 4)\n",
    "\n",
    "unbalanced_loss = load_balancing_loss(unbalanced_logits, 4)\n",
    "balanced_loss = load_balancing_loss(balanced_logits, 4)\n",
    "\n",
    "print(f\"Unbalanced routing loss: {unbalanced_loss.item():.4f}\")\n",
    "print(f\"Balanced routing loss: {balanced_loss.item():.4f}\")\n",
    "print(f\"[OK] Load balancing loss is lower when experts are evenly used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training with MoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_experts=4, top_k=2):\n",
    "        super().__init__()\n",
    "        self.moe = MixtureOfExperts(input_dim, hidden_dim, hidden_dim, num_experts, top_k)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        self.aux_loss_weight = 0.1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.moe(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "# Create data\n",
    "X = torch.randn(500, 20)\n",
    "y = (X[:, 0] + X[:, 1] > 0).long()\n",
    "\n",
    "# Train\n",
    "model = MoEClassifier(20, 64, 2, num_experts=4, top_k=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "utilizations = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.moe.reset_counts()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    \n",
    "    # Main loss\n",
    "    main_loss = criterion(output, y)\n",
    "    \n",
    "    # Load balancing loss\n",
    "    routing_logits = model.moe.router(X)\n",
    "    aux_loss = load_balancing_loss(routing_logits, 4)\n",
    "    \n",
    "    # Total loss\n",
    "    loss = main_loss + 0.1 * aux_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(main_loss.item())\n",
    "    utilizations.append(model.moe.get_expert_utilization().clone())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        acc = (output.argmax(1) == y).float().mean()\n",
    "        print(f\"Epoch {epoch+1}: Loss={main_loss.item():.4f}, Acc={acc:.1%}\")\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(losses)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "\n",
    "util_history = torch.stack(utilizations).numpy()\n",
    "for i in range(4):\n",
    "    ax2.plot(util_history[:, i], label=f'Expert {i+1}')\n",
    "ax2.axhline(0.25, color='k', linestyle='--', label='Ideal (uniform)')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Utilization')\n",
    "ax2.set_title('Expert Utilization')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Capacity Factor\n",
    "\n",
    "The capacity factor limits how many tokens each expert can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_capacity(batch_size, num_experts, top_k, capacity_factor=1.25):\n",
    "    \"\"\"\n",
    "    Compute expert capacity.\n",
    "    \n",
    "    capacity = (batch_size * top_k / num_experts) * capacity_factor\n",
    "    \"\"\"\n",
    "    tokens_per_expert = batch_size * top_k / num_experts\n",
    "    capacity = int(tokens_per_expert * capacity_factor)\n",
    "    return capacity\n",
    "\n",
    "# Example\n",
    "batch_size = 64\n",
    "num_experts = 8\n",
    "top_k = 2\n",
    "\n",
    "for cf in [1.0, 1.25, 1.5, 2.0]:\n",
    "    cap = compute_capacity(batch_size, num_experts, top_k, cf)\n",
    "    print(f\"Capacity factor {cf}: {cap} tokens per expert\")\n",
    "\n",
    "print(f\"\\n[OK] Higher capacity = more tokens can use each expert\")\n",
    "print(f\"[OK] Lower capacity = more dropped tokens but faster compute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key concepts covered:\n",
    "\n",
    "1. **MoE Architecture**: Multiple experts + router for conditional computation\n",
    "2. **Top-K Routing**: Select k experts per input\n",
    "3. **Load Balancing**: Auxiliary loss to ensure even expert utilization\n",
    "4. **Expert Utilization**: Track which experts are being used\n",
    "5. **Capacity Factor**: Control maximum tokens per expert\n",
    "\n",
    "## MoE Benefits\n",
    "\n",
    "- Scale model capacity without proportional compute increase\n",
    "- Different experts can specialize on different data patterns\n",
    "- Sparse activation = efficient inference\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [->] Module 09: Multi-Modal Architectures\n",
    "- [->] Module 10: Capstone Project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
