Metadata-Version: 2.4
Name: ws-foundational-training
Version: 0.1.0
Summary: Interactive educational program for Watts-Strogatz multi-modal neural architectures
Author: Multi-Modal WS Training Team
License: MIT
Project-URL: Homepage, https://github.com/SamuraiBuddha/multi-modal-WS-foundational-training
Project-URL: Documentation, https://github.com/SamuraiBuddha/multi-modal-WS-foundational-training/tree/main/docs
Project-URL: Repository, https://github.com/SamuraiBuddha/multi-modal-WS-foundational-training
Keywords: machine-learning,neural-networks,watts-strogatz,small-world-networks,sparse-training,multi-modal,education
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.1.0
Requires-Dist: torchvision>=0.16.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: scipy>=1.11.0
Requires-Dist: networkx>=3.2
Requires-Dist: matplotlib>=3.8.0
Requires-Dist: plotly>=5.18.0
Requires-Dist: jupyterlab>=4.0.0
Requires-Dist: ipywidgets>=8.1.0
Requires-Dist: tqdm>=4.66.0
Requires-Dist: pyyaml>=6.0.1
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: mypy>=1.6.0; extra == "dev"
Requires-Dist: black>=23.10.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=6.1.0; extra == "dev"
Provides-Extra: full
Requires-Dist: torchaudio>=2.1.0; extra == "full"
Requires-Dist: pandas>=2.1.0; extra == "full"
Requires-Dist: scikit-learn>=1.3.0; extra == "full"
Requires-Dist: seaborn>=0.13.0; extra == "full"

# Multi-Modal Watts-Strogatz Foundational Training

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.1+-ee4c2c.svg)](https://pytorch.org/)

> **An interactive 40-hour educational program teaching neural architecture design from fundamentals through novel Watts-Strogatz small-world topological multi-modal architectures.**

## ğŸ¯ What You'll Learn

This program takes you on a journey from ML basics to designing your own novel neural architecture:

1. **Neural Network Foundations** - Forward passes, activations, from-scratch implementations
2. **Supervised Learning** - Loss functions, gradient descent, backpropagation intuition
3. **Graph Theory** - Networks, adjacency matrices, properties, neural nets as graphs
4. **Network Topology** - ErdÅ‘sâ€“RÃ©nyi, Watts-Strogatz, BarabÃ¡si-Albert, small-world properties
5. **Sparse Networks** - Why sparse, pruning, efficient representations
6. **Unsupervised Learning** - Autoencoders, latent spaces, representation quality
7. **Dynamic Sparse Training** - SET, DEEP R, topology evolution
8. **Modular Architectures** - Mixture of Experts, gating, inter-module communication
9. **Multi-Modal Learning** - Encoders, fusion strategies, cross-modal attention
10. **Capstone** - Build a Segmented Watts-Strogatz Multi-Modal Architecture

## ğŸ—ï¸ The Capstone Architecture

By the end, you'll build this novel architecture combining all learned concepts:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  Visual  â”‚   â”‚   Text   â”‚   â”‚  Audio   â”‚   â† Modality       â”‚
â”‚  â”‚  Module  â”‚   â”‚  Module  â”‚   â”‚  Module  â”‚     Encoders       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚              â”‚   WS Inter-   â”‚  â† Learnable Small-World         â”‚
â”‚              â”‚   Module      â”‚    Connector with Dynamic        â”‚
â”‚              â”‚   Connector   â”‚    Rewiring (Î² learnable)        â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚              â”‚    Fusion     â”‚  â† Cross-Modal Integration       â”‚
â”‚              â”‚    Module     â”‚                                  â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Innovations:**
- Heterogeneous modules for each modality
- Watts-Strogatz topology for inter-module wiring
- Learnable rewiring probability (Î²)
- Dynamic connectivity evolution during training
- 5-10x parameter efficiency vs dense networks

## ğŸ“š Documentation

- **[Product Requirements Document](docs/PRD.md)** - What we're building and why
- **[Technical Specification](docs/SPECIFICATION.md)** - Detailed implementation guide for Claude Code

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone https://github.com/SamuraiBuddha/multi-modal-WS-foundational-training.git
cd multi-modal-WS-foundational-training

# Setup environment
python scripts/setup_environment.py

# Launch JupyterLab
jupyter lab

# Open notebooks/00_setup/00_welcome.ipynb
```

## ğŸ“‹ Prerequisites

- **Hardware**: NVIDIA GPU with 8GB+ VRAM recommended (CPU fallback available)
- **Software**: Python 3.11+
- **Knowledge**: Basic programming concepts (can read Java/C#/HTML)
- **Time**: ~40 hours for complete curriculum

## ğŸ—‚ï¸ Repository Structure

```
multi-modal-WS-foundational-training/
â”œâ”€â”€ notebooks/          # Jupyter notebooks (main content)
â”‚   â”œâ”€â”€ 00_setup/      # Environment setup
â”‚   â”œâ”€â”€ 01_foundations/ # Neural network basics
â”‚   â”œâ”€â”€ 02_supervised/ # Supervised learning
â”‚   â”œâ”€â”€ 03_graphs/     # Graph theory
â”‚   â”œâ”€â”€ 04_topology/   # Network topologies (WS, BA, ER)
â”‚   â”œâ”€â”€ 05_sparse/     # Sparse neural networks
â”‚   â”œâ”€â”€ 06_unsupervised/ # Autoencoders, representation learning
â”‚   â”œâ”€â”€ 07_dynamic_sparse/ # SET, DEEP R algorithms
â”‚   â”œâ”€â”€ 08_modular/    # Mixture of Experts
â”‚   â”œâ”€â”€ 09_multimodal/ # Multi-modal learning
â”‚   â””â”€â”€ 10_capstone/   # Build the full architecture
â”œâ”€â”€ src/               # Python source code
â”œâ”€â”€ web/               # Interactive visualizations (React + D3)
â”œâ”€â”€ tests/             # Test suites
â”œâ”€â”€ configs/           # Configuration files
â””â”€â”€ docs/              # Documentation (PRD, Spec)
```

## âœ¨ Features

### Interactive Learning
- **Network Topology Visualizers** - Watch Watts-Strogatz rewiring in real-time
- **Training Dashboards** - Live loss curves and metrics
- **Topology Evolution** - See connections rewire during training
- **Sandbox Environment** - Design your own architectures visually

### Code-Driven Understanding
- **Scaffolded Exercises** - Partial implementations with TODOs
- **Automated Validation** - Immediate feedback on your code
- **Progressive Hints** - Get unstuck without seeing solutions
- **Solutions Available** - Full implementations when ready

### Conceptual + Implementation Balance
- **Intuitive Explanations** - Analogies and visualizations first
- **Then Implementation** - Build it to understand it
- **Mathematical Depth** - Optional deep-dives for rigor

## ğŸ”¬ Key Research Papers Covered

| Paper | Topic | Implementation |
|-------|-------|----------------|
| Watts & Strogatz 1998 | Small-world networks | Full reproduction |
| Mocanu et al. 2018 | SET algorithm | Full implementation |
| Bellec et al. 2017 | DEEP R | Full implementation |
| Zhang et al. 2023 | BSW/CHT | Partial implementation |
| Chen et al. 2024 | MacNet (SW collaboration) | Conceptual coverage |

## ğŸ› ï¸ Technology Stack

**Primary (Full Coverage):**
- Python 3.11+
- PyTorch 2.x
- JupyterLab 4.x
- NetworkX
- React + D3.js + Plotly

**Secondary (Examples):**
- TensorFlow/Keras
- JAX/Flax
- Pure NumPy

## ğŸ“ˆ Learning Outcomes

After completing this program, you will be able to:

1. âœ… Explain why small-world topology improves neural network efficiency
2. âœ… Implement SET and DEEP R sparse training algorithms
3. âœ… Design modular neural architectures with learned gating
4. âœ… Build multi-modal fusion systems
5. âœ… Create your own segmented WS architecture
6. âœ… Read and understand research papers on sparse training

## ğŸ¤ Contributing

Contributions welcome! See documentation for guidelines.

## ğŸ“„ License

MIT License

## ğŸ™ Acknowledgments

This educational program synthesizes research from:
- Watts & Strogatz (small-world networks)
- Mocanu, Mocanu, et al. (SET algorithm)
- Bellec et al. (DEEP R)
- Network science and neuroscience communities

---

**Ready to learn?** Start with `notebooks/00_setup/00_welcome.ipynb`!
